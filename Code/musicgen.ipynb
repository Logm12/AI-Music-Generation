{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T11:01:03.555949Z",
     "iopub.status.busy": "2025-06-23T11:01:03.555645Z",
     "iopub.status.idle": "2025-06-23T11:01:13.988454Z",
     "shell.execute_reply": "2025-06-23T11:01:13.987242Z",
     "shell.execute_reply.started": "2025-06-23T11:01:03.555916Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mido\n",
      "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mido) (24.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\n",
      "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mido\n",
      "Successfully installed mido-1.3.3\n",
      "Collecting miditok\n",
      "  Downloading miditok-3.0.6-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from miditok) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from miditok) (1.26.4)\n",
      "Collecting symusic>=0.5.0 (from miditok)\n",
      "  Downloading symusic-0.5.8-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from miditok) (0.21.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from miditok) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->miditok) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->miditok) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->miditok) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->miditok) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->miditok) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->miditok) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19->miditok) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19->miditok) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19->miditok) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19->miditok) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19->miditok) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19->miditok) (2.4.1)\n",
      "Collecting pySmartDL (from symusic>=0.5.0->miditok)\n",
      "  Downloading pySmartDL-1.3.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from symusic>=0.5.0->miditok) (4.3.6)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19->miditok) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19->miditok) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19->miditok) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19->miditok) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (2025.1.31)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19->miditok) (2024.2.0)\n",
      "Downloading miditok-3.0.6-py3-none-any.whl (158 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.9/158.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading symusic-0.5.8-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pySmartDL-1.3.4-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: pySmartDL, symusic, miditok\n",
      "Successfully installed miditok-3.0.6 pySmartDL-1.3.4 symusic-0.5.8\n"
     ]
    }
   ],
   "source": [
    "!pip install mido numpy torch\n",
    "!pip install miditok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T13:34:45.804387Z",
     "iopub.status.busy": "2025-06-23T13:34:45.804051Z",
     "iopub.status.idle": "2025-06-23T13:34:45.833786Z",
     "shell.execute_reply": "2025-06-23T13:34:45.833078Z",
     "shell.execute_reply.started": "2025-06-23T13:34:45.804360Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- System Info ---\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA is available: True\n",
      "Device name: Tesla T4\n",
      "Using device: cuda\n",
      "---------------------\n",
      "\n",
      "Tokenizer initialized. Vocabulary size: 455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Imports\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  # <--- ADD THIS LINE\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DataCollator\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# 3. System and Device Information\n",
    "print(\"--- System Info ---\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "IS_CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "print(f\"CUDA is available: {IS_CUDA_AVAILABLE}\")\n",
    "if IS_CUDA_AVAILABLE:\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "DEVICE = torch.device(\"cuda\" if IS_CUDA_AVAILABLE else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(\"---------------------\\n\")\n",
    "\n",
    "# 4. Tokenizer Configuration\n",
    "NUM_BINS = 10\n",
    "V_TOKENS = [f\"<v_{i}>\" for i in range(NUM_BINS)]\n",
    "A_TOKENS = [f\"<a_{i}>\" for i in range(NUM_BINS)]\n",
    "BASE_SPECIAL_TOKENS = [\"PAD\", \"BOS\", \"EOS\", \"MASK\"]\n",
    "ALL_SPECIAL_TOKENS = BASE_SPECIAL_TOKENS + V_TOKENS + A_TOKENS\n",
    "\n",
    "config = TokenizerConfig(\n",
    "    pitch_range=(21, 109),\n",
    "    beat_res={(0, 4): 8, (4, 12): 4},\n",
    "    num_velocities=32,\n",
    "    special_tokens=ALL_SPECIAL_TOKENS,\n",
    "    use_chords=True,\n",
    "    use_rests=True,\n",
    "    use_tempos=True,\n",
    "    use_time_signatures=True,\n",
    "    use_programs=False\n",
    ")\n",
    "tokenizer = REMI(config)\n",
    "print(f\"Tokenizer initialized. Vocabulary size: {len(tokenizer)}\\n\")\n",
    "\n",
    "# 5. Model Hyperparameter Configurations\n",
    "class SmallModelConfig:\n",
    "    def __init__(self, vocab_size, max_seq_len):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = 512    # Tăng từ 256\n",
    "        self.nhead = 8        # Tăng từ 4\n",
    "        self.d_hid = 2048     # Tăng từ 1024\n",
    "        self.nlayers = 6      # Tăng từ 4\n",
    "        self.dropout = 0.2\n",
    "\n",
    "# 6. Global Utility Functions\n",
    "def quantize_va(v, a, num_bins=10):\n",
    "    \"\"\"Quantizes Valence/Arousal values into token strings.\"\"\"\n",
    "    v_bin = int((v + 1.0) / 2.0 * (num_bins - 1))\n",
    "    a_bin = int((a + 1.0) / 2.0 * (num_bins - 1))\n",
    "    v_bin = max(0, min(num_bins - 1, v_bin))\n",
    "    a_bin = max(0, min(num_bins - 1, a_bin))\n",
    "    return f\"<v_{v_bin}>\", f\"<a_{a_bin}>\"\n",
    "\n",
    "# 7. Dataset Classes\n",
    "class VAMIDI_Dataset(Dataset): # For our labelled, fine-tuning data\n",
    "    def __init__(self, root_dir, tokenizer, va_data_df, max_seq_len, num_va_bins=10, stride=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.root_dir = root_dir\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_va_bins = num_va_bins\n",
    "        self.stride = stride\n",
    "        self.samples = []\n",
    "        va_data_df['filename'] = va_data_df['midi'].apply(lambda path: os.path.basename(path))\n",
    "        self.va_lookup = va_data_df.set_index('filename').to_dict('index')\n",
    "        self._prepare_data()\n",
    "    def _prepare_data(self):\n",
    "        print(f\"Starting fine-tuning data preparation with chunk size {self.max_seq_len}...\")\n",
    "        available_files = {f for f in os.listdir(self.root_dir) if f.endswith(('.mid', '.midi'))}\n",
    "        files_to_process = [f for f in available_files if f in self.va_lookup]\n",
    "        for filename in tqdm(files_to_process, desc=\"Processing labelled MIDI files\"):\n",
    "            midi_path = os.path.join(self.root_dir, filename)\n",
    "            try: tokens = self.tokenizer(midi_path)\n",
    "            except Exception as e: continue\n",
    "            if len(tokens) == 0 or len(tokens[0].ids) == 0: continue\n",
    "            token_ids = tokens[0].ids\n",
    "            v, a = self.va_lookup[filename]['valence'], self.va_lookup[filename]['arousal']\n",
    "            v_token_str, a_token_str = quantize_va(v, a, num_bins=self.num_va_bins)\n",
    "            v_token_id, a_token_id = self.tokenizer[v_token_str], self.tokenizer[a_token_str]\n",
    "            bos_id, eos_id = self.tokenizer['BOS_None'], self.tokenizer['EOS_None']\n",
    "            full_ids = [v_token_id, a_token_id] + token_ids\n",
    "            start = 0\n",
    "            while True:\n",
    "                end = start + self.max_seq_len - 2\n",
    "                chunk = full_ids[start:end]\n",
    "                final_sequence = [bos_id] + chunk + [eos_id]\n",
    "                self.samples.append(torch.tensor(final_sequence, dtype=torch.long))\n",
    "                if end >= len(full_ids): break\n",
    "                start += self.stride\n",
    "        print(f\"Fine-tuning data preparation complete. Total chunks: {len(self.samples)}\")\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return {'input_ids': self.samples[idx]}\n",
    "\n",
    "class MIDIDataset_Pretrain(Dataset): # For the large, unlabelled pre-training data\n",
    "    def __init__(self, root_dir, tokenizer, max_seq_len, stride=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.stride = stride\n",
    "        self.samples = []\n",
    "        self._prepare_data(root_dir)\n",
    "    def _prepare_data(self, root_dir):\n",
    "        print(f\"Starting pre-training data preparation with chunk size {self.max_seq_len}...\")\n",
    "        paths = list(Path(root_dir).glob('**/*.mid')) + list(Path(root_dir).glob('**/*.midi'))\n",
    "        print(f\"Found {len(paths)} MIDI files for pre-training in {root_dir}.\")\n",
    "        for path in tqdm(paths, desc=\"Processing pre-training MIDI files\"):\n",
    "            try: tokens = self.tokenizer(str(path))\n",
    "            except Exception as e: continue\n",
    "            if len(tokens) == 0 or len(tokens[0].ids) == 0: continue\n",
    "            token_ids = tokens[0].ids\n",
    "            bos_id, eos_id = self.tokenizer['BOS_None'], self.tokenizer['EOS_None']\n",
    "            start = 0\n",
    "            while True:\n",
    "                end = start + self.max_seq_len - 2\n",
    "                chunk = token_ids[start:end]\n",
    "                final_sequence = [bos_id] + chunk + [eos_id]\n",
    "                self.samples.append(torch.tensor(final_sequence, dtype=torch.long))\n",
    "                if end >= len(token_ids): break\n",
    "                start += self.stride\n",
    "        print(f\"Pre-training data preparation complete. Total chunks: {len(self.samples)}\")\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return {'input_ids': self.samples[idx]}\n",
    "\n",
    "# 8. Model Architecture\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Give the positional encoding buffer a little extra room to avoid off-by-one errors during generation.\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000): \n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # This line is now safe because self.pe is much longer than x\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_encoder = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(config.d_model, config.dropout, max_len=config.max_seq_len + 5)\n",
    "        \n",
    "        decoder_layer = TransformerDecoderLayer(d_model=config.d_model, nhead=config.nhead, dim_feedforward=config.d_hid, dropout=config.dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=config.nlayers)\n",
    "        self.output_head = nn.Linear(config.d_model, config.vocab_size)\n",
    "        self.init_weights()\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.token_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_head.bias.data.zero_()\n",
    "        self.output_head.weight.data.uniform_(-initrange, initrange)\n",
    "    def forward(self, src: torch.Tensor, src_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        seq_len = src.size(1)\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len, device=src.device)\n",
    "        \n",
    "        src_emb = self.token_encoder(src) * math.sqrt(self.config.d_model)\n",
    "        src_emb = src_emb.permute(1, 0, 2)\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        src_emb = src_emb.permute(1, 0, 2)\n",
    "        \n",
    "        output = self.transformer_decoder(tgt=src_emb, memory=src_emb, tgt_mask=causal_mask, tgt_key_padding_mask=src_padding_mask)\n",
    "        output = self.output_head(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T13:34:52.101759Z",
     "iopub.status.busy": "2025-06-23T13:34:52.101482Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pre-training data preparation with chunk size 1024...\n",
      "Found 1282 MIDI files for pre-training in /kaggle/input/themaestrodatasetv2/maestro-v2.0.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e04b45187145d9b0218082cc632a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing pre-training MIDI files:   0%|          | 0/1282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training data preparation complete. Total chunks: 52096\n",
      "--- Starting Pre-training on MAESTRO (52096 samples) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ca22536e4c45f7b7f15b4ef620fb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-training Epoch 1:   0%|          | 0/6512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training Epoch 1 | Avg Loss: 2.5461\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc67efab9dc64b4fac5a1a9bc0c6ebe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-training Epoch 2:   0%|          | 0/6512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training Epoch 2 | Avg Loss: 0.2228\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d788f220df4e32b2a94c3dda102bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-training Epoch 3:   0%|          | 0/6512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Define Parameters and Paths\n",
    "CHUNK_SIZE = 1024\n",
    "PRETRAIN_BATCH_SIZE = 8\n",
    "MAESTRO_ROOT_PATH = \"/kaggle/input/themaestrodatasetv2/maestro-v2.0.0\"\n",
    "PRETRAIN_SAVE_PATH = \"/kaggle/working/models/music_transformer_pretrained.pth\"\n",
    "NUM_PRETRAIN_EPOCHS = 3\n",
    "\n",
    "# 2. Create Dataset and DataLoader\n",
    "data_collator = DataCollator(pad_token_id=tokenizer['PAD_None'])\n",
    "pretrain_dataset = MIDIDataset_Pretrain(MAESTRO_ROOT_PATH, tokenizer, max_seq_len=CHUNK_SIZE)\n",
    "pretrain_dataloader = DataLoader(\n",
    "    pretrain_dataset,\n",
    "    batch_size=PRETRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 3. Initialize Model and Optimizer\n",
    "model_config = SmallModelConfig(vocab_size=len(tokenizer), max_seq_len=CHUNK_SIZE)\n",
    "model = MusicTransformer(model_config).to(DEVICE)\n",
    "# Trong Cell 2, dòng optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5) # Giảm learning rate\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer['PAD_None'])\n",
    "\n",
    "# 4. Pre-training Loop\n",
    "print(f\"--- Starting Pre-training on MAESTRO ({len(pretrain_dataset)} samples) ---\")\n",
    "os.makedirs(\"/kaggle/working/models\", exist_ok=True)\n",
    "model.train()\n",
    "for epoch in range(1, NUM_PRETRAIN_EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(pretrain_dataloader, desc=f\"Pre-training Epoch {epoch}\"):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        padding_mask = (attention_mask == 0)\n",
    "\n",
    "        logits = model(src=input_ids, src_padding_mask=padding_mask)\n",
    "\n",
    "        shifted_logits = logits[:, :-1, :].contiguous()\n",
    "        shifted_labels = input_ids[:, 1:].contiguous()\n",
    "\n",
    "        # Flatten the tokens\n",
    "        loss = loss_fn(\n",
    "            shifted_logits.view(-1, model_config.vocab_size),\n",
    "            shifted_labels.view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(pretrain_dataloader)\n",
    "    print(f\"Pre-training Epoch {epoch} | Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 5. Save Pre-trained Model\n",
    "torch.save(model.state_dict(), PRETRAIN_SAVE_PATH)\n",
    "print(f\"Pre-trained model saved to {PRETRAIN_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from miditok import TokSequence\n",
    "\n",
    "# 1. Generation Parameters\n",
    "# We might need to adjust these if the model repeats itself.\n",
    "MAX_GEN_LEN = 1024\n",
    "TEMPERATURE = 1.0\n",
    "TOP_P = 0.95\n",
    "REPETITION_PENALTY = 1.2\n",
    "PRETRAINED_MODEL_PATH = \"/kaggle/working/models/music_transformer_pretrained.pth\"\n",
    "\n",
    "# 2. Load the Pre-trained Model\n",
    "print(\"--- Loading Pre-trained Model ---\")\n",
    "# Ensure model_config is defined from a previous cell\n",
    "model_gen_pretrain = MusicTransformer(model_config).to(DEVICE)\n",
    "model_gen_pretrain.load_state_dict(torch.load(PRETRAINED_MODEL_PATH, map_location=DEVICE, weights_only=True))\n",
    "model_gen_pretrain.eval()\n",
    "print(\"Pre-trained model loaded successfully.\")\n",
    "\n",
    "# 3. Generation Function (Simplified for pre-trained model)\n",
    "def generate_from_pretrained(model, tokenizer, max_len, temperature, top_p, repetition_penalty):\n",
    "    print(f\"\\nStarting generation from pre-trained model...\")\n",
    "    \n",
    "    # --- The prompt is very simple: just the beginning of a sequence ---\n",
    "    bos_id = tokenizer['BOS_None']\n",
    "    input_ids = [bos_id]\n",
    "    # ------------------------------------------------------------------\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(max_len), desc=\"Generating tokens\"):\n",
    "            input_tensor = torch.tensor([input_ids], dtype=torch.long, device=DEVICE)\n",
    "            logits = model(input_tensor)\n",
    "            last_token_logits = logits[0, -1, :]\n",
    "\n",
    "            # Apply Repetition Penalty\n",
    "            # Penalize the last 20 tokens to encourage variety\n",
    "            if len(input_ids) > 1:\n",
    "                for token_id in set(input_ids[-20:]):\n",
    "                    last_token_logits[token_id] /= repetition_penalty\n",
    "\n",
    "            # Apply Temperature\n",
    "            scaled_logits = last_token_logits / temperature\n",
    "            \n",
    "            # Apply Top-p (Nucleus) Sampling\n",
    "            sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            \n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            scaled_logits[indices_to_remove] = -float('inf')\n",
    "\n",
    "            probs = torch.nn.functional.softmax(scaled_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            if next_token_id.item() == tokenizer['EOS_None']:\n",
    "                print(\"\\nEnd-of-Sequence token generated. Stopping.\")\n",
    "                break\n",
    "            \n",
    "            input_ids.append(next_token_id.item())\n",
    "            \n",
    "    print(f\"Generated {len(input_ids)} tokens.\")\n",
    "    print(\"\\n--- 40 Token đầu tiên được sinh ra ---\")\n",
    "    generated_token_strings = [tokenizer[id_] for id_ in input_ids[:40]]\n",
    "    print(generated_token_strings)\n",
    "    return input_ids\n",
    "\n",
    "# 4. Generate, Convert to MIDI, and Save\n",
    "pretrained_tokens = generate_from_pretrained(\n",
    "    model_gen_pretrain, tokenizer, \n",
    "    max_len=MAX_GEN_LEN, temperature=TEMPERATURE, top_p=TOP_P, repetition_penalty=REPETITION_PENALTY\n",
    ")\n",
    "\n",
    "if pretrained_tokens:\n",
    "    tok_seq = TokSequence(ids=pretrained_tokens)\n",
    "    generated_midi_pretrained = tokenizer.decode([tok_seq])\n",
    "    output_path_pretrained = \"/kaggle/working/pretrained_generated_music.mid\"\n",
    "    generated_midi_pretrained.dump_midi(output_path_pretrained)\n",
    "    print(f\"\\nMusic generated from pre-trained model saved to: {output_path_pretrained}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T13:25:56.443847Z",
     "iopub.status.busy": "2025-06-23T13:25:56.443561Z",
     "iopub.status.idle": "2025-06-23T13:28:14.561345Z",
     "shell.execute_reply": "2025-06-23T13:28:14.560426Z",
     "shell.execute_reply.started": "2025-06-23T13:25:56.443827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning data preparation with chunk size 1024...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5753348604940a282a4921fe44cb40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing labelled MIDI files:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning data preparation complete. Total chunks: 1012\n",
      "Successfully loaded pre-trained weights for fine-tuning.\n",
      "--- Starting Fine-tuning (1012 samples) for 4 epochs ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd02cca7fbd74f4092939e7b8e2a30f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fine-tuning Epoch 1:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 1 | Avg Loss: 0.0261\n",
      "Model saved after epoch 1 to /kaggle/working/models/music_transformer_finetuned.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa1a562fb4540368a5d2419a3799c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fine-tuning Epoch 2:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 2 | Avg Loss: 0.0150\n",
      "Model saved after epoch 2 to /kaggle/working/models/music_transformer_finetuned.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c18331e92f4de598811e1888473008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fine-tuning Epoch 3:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 3 | Avg Loss: 0.0121\n",
      "Model saved after epoch 3 to /kaggle/working/models/music_transformer_finetuned.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d550e635df40e2a0022e97a9c2d342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fine-tuning Epoch 4:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 4 | Avg Loss: 0.0110\n",
      "Model saved after epoch 4 to /kaggle/working/models/music_transformer_finetuned.pth\n",
      "--- Fine-tuning Complete ---\n"
     ]
    }
   ],
   "source": [
    "# 1. Define Parameters and Paths\n",
    "FINETUNE_BATCH_SIZE = 8\n",
    "LABEL_FILE = \"/kaggle/input/emodata/vgmidi-master/vgmidi_labelled.csv\"\n",
    "MIDI_DIR_CORRECTED = \"/kaggle/input/emodata/vgmidi-master/labelled/phrases/phrases\"\n",
    "FINETUNE_SAVE_PATH = \"/kaggle/working/models/music_transformer_finetuned.pth\"\n",
    "\n",
    "NUM_FINETUNE_EPOCHS = 4\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "FINETUNE_LR = 5e-5 # Use a smaller learning rate\n",
    "\n",
    "# 2. Create Fine-tuning Dataset and DataLoader\n",
    "labels_df = pd.read_csv(LABEL_FILE)\n",
    "finetune_dataset = VAMIDI_Dataset(\n",
    "    root_dir=MIDI_DIR_CORRECTED,\n",
    "    tokenizer=tokenizer,\n",
    "    va_data_df=labels_df.copy(),\n",
    "    max_seq_len=CHUNK_SIZE\n",
    ")\n",
    "finetune_dataloader = DataLoader(\n",
    "    finetune_dataset,\n",
    "    batch_size=FINETUNE_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(PRETRAIN_SAVE_PATH, weights_only=True))\n",
    "print(\"Successfully loaded pre-trained weights for fine-tuning.\")\n",
    "optimizer = AdamW(model.parameters(), lr=FINETUNE_LR, weight_decay=0.01)\n",
    "\n",
    "# 4. Fine-tuning Loop\n",
    "print(f\"--- Starting Fine-tuning ({len(finetune_dataset)} samples) for {NUM_FINETUNE_EPOCHS} epochs ---\")\n",
    "best_finetune_loss = float('inf')\n",
    "model.train()\n",
    "for epoch in range(1, NUM_FINETUNE_EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(finetune_dataloader, desc=f\"Fine-tuning Epoch {epoch}\"):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        padding_mask = (attention_mask == 0)\n",
    "\n",
    "        # Corrected loss calculation\n",
    "        logits = model(src=input_ids, src_padding_mask=padding_mask)\n",
    "        shifted_logits = logits[:, :-1, :].contiguous()\n",
    "        shifted_labels = input_ids[:, 1:].contiguous()\n",
    "\n",
    "        loss = loss_fn(\n",
    "            shifted_logits.view(-1, model_config.vocab_size),\n",
    "            shifted_labels.view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(finetune_dataloader)\n",
    "    print(f\"Fine-tuning Epoch {epoch} | Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # We will always save the latest model in this shortened training schedule.\n",
    "    # The 'best_loss' logic is less critical here since we are manually stopping early.\n",
    "    torch.save(model.state_dict(), FINETUNE_SAVE_PATH)\n",
    "    print(f\"Model saved after epoch {epoch} to {FINETUNE_SAVE_PATH}\")\n",
    "\n",
    "print(\"--- Fine-tuning Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T13:28:30.223683Z",
     "iopub.status.busy": "2025-06-23T13:28:30.223380Z",
     "iopub.status.idle": "2025-06-23T13:28:40.968258Z",
     "shell.execute_reply": "2025-06-23T13:28:40.967287Z",
     "shell.execute_reply.started": "2025-06-23T13:28:30.223658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tải mô hình đã Fine-tune ---\n",
      "Tải mô hình thành công.\n",
      "\n",
      "Bắt đầu sinh nhạc cho Valence=0.8, Arousal=0.7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-34d897fdee27>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_gen.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=DEVICE))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cfea1592df4536973e70e803c5e3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Đang sinh token:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã sinh ra 1027 token.\n",
      "\n",
      "--- 40 Token đầu tiên được sinh ra ---\n",
      "['BOS_None', '<v_8>', '<a_7>', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70', 'Pitch_70']\n",
      "Nhạc vui đã được lưu tại: /kaggle/working/happy_generated_music.mid\n",
      "\n",
      "Bắt đầu sinh nhạc cho Valence=-0.8, Arousal=-0.6...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5555009be30b4afe93df353184856343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Đang sinh token:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã sinh ra 1027 token.\n",
      "\n",
      "--- 40 Token đầu tiên được sinh ra ---\n",
      "['BOS_None', '<v_0>', '<a_1>', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72', 'Pitch_72']\n",
      "Nhạc buồn đã được lưu tại: /kaggle/working/sad_generated_music.mid\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Sinh nhạc (v3 - Lấy mẫu Nâng cao)\n",
    "# =========================================\n",
    "\n",
    "from miditok import TokSequence\n",
    "\n",
    "# 1. Các tham số Sinh nhạc\n",
    "TEMPERATURE = 1.5  # Tăng mạnh để tăng tính ngẫu nhiên\n",
    "TOP_P = 0.9        # Giảm nhẹ\n",
    "REPETITION_PENALTY # Hình phạt cho việc lặp lại token. > 1.0 sẽ hạn chế lặp lại.\n",
    "\n",
    "FINAL_MODEL_PATH = \"/kaggle/working/models/music_transformer_finetuned.pth\"\n",
    "\n",
    "# 2. Tải mô hình đã Fine-tune\n",
    "print(\"--- Tải mô hình đã Fine-tune ---\")\n",
    "model_gen = MusicTransformer(model_config).to(DEVICE)\n",
    "model_gen.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=DEVICE))\n",
    "model_gen.eval()\n",
    "print(\"Tải mô hình thành công.\")\n",
    "\n",
    "# 3. Hàm Sinh nhạc NÂNG CAO\n",
    "def generate_music(model, tokenizer, prompt_v, prompt_a, max_len, temperature, top_p, repetition_penalty):\n",
    "    print(f\"\\nBắt đầu sinh nhạc cho Valence={prompt_v}, Arousal={prompt_a}...\")\n",
    "    v_token_str, a_token_str = quantize_va(prompt_v, prompt_a, num_bins=NUM_BINS)\n",
    "    v_token_id, a_token_id = tokenizer[v_token_str], tokenizer[a_token_str]\n",
    "    bos_id = tokenizer['BOS_None']\n",
    "    input_ids = [bos_id, v_token_id, a_token_id]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(max_len), desc=\"Đang sinh token\"):\n",
    "            input_tensor = torch.tensor([input_ids], dtype=torch.long, device=DEVICE)\n",
    "            logits = model(input_tensor)\n",
    "            last_token_logits = logits[0, -1, :]\n",
    "\n",
    "            # --- Áp dụng Hình phạt Lặp lại ---\n",
    "            # Trừng phạt các token trong 20 bước gần nhất\n",
    "            for token_id in set(input_ids[-20:]):\n",
    "                last_token_logits[token_id] /= repetition_penalty\n",
    "\n",
    "            # --- Áp dụng Nhiệt độ (Temperature) ---\n",
    "            scaled_logits = last_token_logits / temperature\n",
    "            \n",
    "            # --- Áp dụng Lấy mẫu Top-p (Nucleus) ---\n",
    "            sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            \n",
    "            # Loại bỏ các token có xác suất tích lũy trên ngưỡng\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            # Dịch chỉ số sang phải để giữ lại token đầu tiên vượt ngưỡng\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            scaled_logits[indices_to_remove] = -float('inf')\n",
    "\n",
    "            # Lấy mẫu từ phân phối đã được lọc\n",
    "            probs = torch.nn.functional.softmax(scaled_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            if next_token_id.item() == tokenizer['EOS_None']:\n",
    "                print(\"\\nToken kết thúc chuỗi được sinh ra. Dừng lại.\")\n",
    "                break\n",
    "            \n",
    "            input_ids.append(next_token_id.item())\n",
    "            \n",
    "    print(f\"Đã sinh ra {len(input_ids)} token.\")\n",
    "    print(\"\\n--- 40 Token đầu tiên được sinh ra ---\")\n",
    "    generated_token_strings = [tokenizer[id_] for id_ in input_ids[:40]]\n",
    "    print(generated_token_strings)\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "# 4. Sinh nhạc, chuyển đổi sang MIDI và Lưu\n",
    "# --- Sinh một bản nhạc Vui/Sôi động ---\n",
    "happy_tokens_ids = generate_music(\n",
    "    model_gen, tokenizer, prompt_v=0.8, prompt_a=0.7, \n",
    "    max_len=MAX_GEN_LEN, temperature=TEMPERATURE, top_p=TOP_P, repetition_penalty=REPETITION_PENALTY\n",
    ")\n",
    "if happy_tokens_ids:\n",
    "    tok_seq = TokSequence(ids=happy_tokens_ids)\n",
    "    generated_midi_happy = tokenizer.decode([tok_seq])\n",
    "    output_path_happy = \"/kaggle/working/happy_generated_music.mid\"\n",
    "    generated_midi_happy.dump_midi(output_path_happy)\n",
    "    print(f\"Nhạc vui đã được lưu tại: {output_path_happy}\")\n",
    "\n",
    "# --- Sinh một bản nhạc Buồn/Lặng ---\n",
    "sad_tokens_ids = generate_music(\n",
    "    model_gen, tokenizer, prompt_v=-0.8, prompt_a=-0.6, \n",
    "    max_len=MAX_GEN_LEN, temperature=TEMPERATURE, top_p=TOP_P, repetition_penalty=REPETITION_PENALTY\n",
    ")\n",
    "if sad_tokens_ids:\n",
    "    tok_seq = TokSequence(ids=sad_tokens_ids)\n",
    "    generated_midi_sad = tokenizer.decode([tok_seq])\n",
    "    output_path_sad = \"/kaggle/working/sad_generated_music.mid\"\n",
    "    generated_midi_sad.dump_midi(output_path_sad)\n",
    "    print(f\"Nhạc buồn đã được lưu tại: {output_path_sad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T11:52:01.328175Z",
     "iopub.status.busy": "2025-06-23T11:52:01.327831Z",
     "iopub.status.idle": "2025-06-23T11:52:05.056589Z",
     "shell.execute_reply": "2025-06-23T11:52:05.055574Z",
     "shell.execute_reply.started": "2025-06-23T11:52:01.328148Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting midi-player\n",
      "  Downloading midi_player-0.5.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading midi_player-0.5.1-py3-none-any.whl (6.4 kB)\n",
      "Installing collected packages: midi-player\n",
      "Successfully installed midi-player-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install midi-player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T11:57:05.040433Z",
     "iopub.status.busy": "2025-06-23T11:57:05.040017Z",
     "iopub.status.idle": "2025-06-23T11:57:05.046826Z",
     "shell.execute_reply": "2025-06-23T11:57:05.045985Z",
     "shell.execute_reply.started": "2025-06-23T11:57:05.040403Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"&lt;script src=&quot;https://cdn.jsdelivr.net/combine/npm/tone@14.7.58,npm/@magenta/music@1.23.1/es6/core.js,npm/focus-visible@5,npm/html-midi-player@1.5.0&quot;&gt;&lt;/script&gt;  &amp;nbsp; &lt;a href=&quot;data:audio/midi;base64,TVRoZAAAAAYAAQABABBNVHJrAAAAEwD/WAQEAhgIAP9RAweMWgH/LwA=&quot; target=&quot;_blank&quot;&gt;Download MIDI&lt;/a&gt;&lt;br&gt;\n",
       "            &lt;midi-player src=&quot;data:audio/midi;base64,TVRoZAAAAAYAAQABABBNVHJrAAAAEwD/WAQEAhgIAP9RAweMWgH/LwA=&quot; sound-font visualizer=&quot;#myVisualizer&quot;&gt;&lt;/midi-player&gt;\n",
       "            &lt;midi-visualizer type=&quot;piano-roll&quot; id=&quot;myVisualizer&quot; style=&quot;background: #fff;&quot;&gt;&lt;/midi-visualizer&gt;\" width=\"100%\" height=\"400\"\n",
       "            style=\"border:none !important;\"\n",
       "            \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\">'\n",
       "            </iframe>"
      ],
      "text/plain": [
       "<midi_player.midi_player.MIDIPlayer at 0x7948bfa8c580>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from midi_player import MIDIPlayer\n",
    "\n",
    "# Replace with your file path\n",
    "midi_path = \"/kaggle/working/happy_generated_music.mid\"\n",
    "\n",
    "# This embeds a clickable MIDI player widget with default dimensions\n",
    "MIDIPlayer(midi_path, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T11:52:54.116748Z",
     "iopub.status.busy": "2025-06-23T11:52:54.116437Z",
     "iopub.status.idle": "2025-06-23T11:52:54.140946Z",
     "shell.execute_reply": "2025-06-23T11:52:54.140323Z",
     "shell.execute_reply.started": "2025-06-23T11:52:54.116723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from mido import MidiFile, MidiTrack, Message, MetaMessage\n",
    "\n",
    "mid = MidiFile(\"/kaggle/working/happy_generated_music.mid\")\n",
    "\n",
    "if not any(len(t) for t in mid.tracks):\n",
    "    track = MidiTrack()\n",
    "    track.append(MetaMessage('end_of_track', time=0))\n",
    "    mid.tracks.append(track)\n",
    "\n",
    "for t in mid.tracks:\n",
    "    if t and t[-1].type != 'end_of_track':\n",
    "        t.append(MetaMessage('end_of_track', time=0))\n",
    "\n",
    "mid.save(\"fixed.mid\")\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T11:53:45.592648Z",
     "iopub.status.busy": "2025-06-23T11:53:45.592325Z",
     "iopub.status.idle": "2025-06-23T11:54:23.214906Z",
     "shell.execute_reply": "2025-06-23T11:54:23.213871Z",
     "shell.execute_reply.started": "2025-06-23T11:53:45.592625Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting midi2audio\n",
      "  Downloading midi2audio-0.1.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
      "Downloading midi2audio-0.1.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: midi2audio\n",
      "Successfully installed midi2audio-0.1.1\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \u001b[0m\n",
      "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \u001b[0m\u001b[33m\u001b[33m\n",
      "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \u001b[0m\u001b[33m\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]      \u001b[0m\n",
      "Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [79.8 kB]\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]3m\u001b[33m\n",
      "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,798 kB]\n",
      "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,703 kB]\n",
      "Get:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
      "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,053 kB] \u001b[0m\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,340 kB][33m\u001b[33m\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,561 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]33m\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,024 kB]3m\u001b[33m\u001b[33m\n",
      "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,747 kB][0m\u001b[33m\n",
      "Get:22 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.2 kB]\n",
      "Get:23 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.6 kB]\n",
      "Get:24 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.1 kB]\n",
      "Get:25 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]3m\n",
      "Get:26 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,253 kB]\n",
      "Get:27 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,532 kB]\n",
      "Fetched 32.9 MB in 3s (12.4 MB/s)33m                         \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "280 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "The following additional packages will be installed:\n",
      "  libevdev2 libgudev-1.0-0 libinput-bin libinput10 libinstpatch-1.0-2 libmd4c0\n",
      "  libmtdev1 libqt5core5a libqt5dbus5 libqt5gui5 libqt5network5 libqt5svg5\n",
      "  libqt5widgets5 libwacom-bin libwacom-common libwacom9 libxcb-icccm4\n",
      "  libxcb-image0 libxcb-keysyms1 libxcb-render-util0 libxcb-util1\n",
      "  libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxkbcommon-x11-0 qsynth\n",
      "  qt5-gtk-platformtheme qttranslations5-l10n\n",
      "Suggested packages:\n",
      "  fluid-soundfont-gs qt5-image-formats-plugins qtwayland5 jackd\n",
      "The following NEW packages will be installed:\n",
      "  fluid-soundfont-gm fluidsynth libevdev2 libfluidsynth3 libgudev-1.0-0\n",
      "  libinput-bin libinput10 libinstpatch-1.0-2 libmd4c0 libmtdev1 libqt5core5a\n",
      "  libqt5dbus5 libqt5gui5 libqt5network5 libqt5svg5 libqt5widgets5 libwacom-bin\n",
      "  libwacom-common libwacom9 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
      "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
      "  libxkbcommon-x11-0 qsynth qt5-gtk-platformtheme qttranslations5-l10n\n",
      "0 upgraded, 31 newly installed, 0 to remove and 280 not upgraded.\n",
      "Need to get 142 MB of archives.\n",
      "After this operation, 201 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5core5a amd64 5.15.3+dfsg-2ubuntu0.2 [2,006 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevdev2 amd64 1.12.1+dfsg-1 [39.5 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmtdev1 amd64 1.1.6-1build4 [14.5 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-common all 2.2.0-1 [54.3 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom9 amd64 2.2.0-1 [22.0 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput-bin amd64 1.20.0-1ubuntu0.3 [19.9 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput10 amd64 1.20.0-1ubuntu0.3 [131 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmd4c0 amd64 0.4.8-1 [42.0 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5dbus5 amd64 5.15.3+dfsg-2ubuntu0.2 [222 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5network5 amd64 5.15.3+dfsg-2ubuntu0.2 [731 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5gui5 amd64 5.15.3+dfsg-2ubuntu0.2 [3,722 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5widgets5 amd64 5.15.3+dfsg-2ubuntu0.2 [2,561 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5svg5 amd64 5.15.3-1 [149 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fluid-soundfont-gm all 3.1-5.3 [130 MB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libinstpatch-1.0-2 amd64 1.1.6-1 [240 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfluidsynth3 amd64 2.2.5-1 [246 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fluidsynth amd64 2.2.5-1 [27.4 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-bin amd64 2.2.0-1 [13.6 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qsynth amd64 0.9.6-1 [305 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 qt5-gtk-platformtheme amd64 5.15.3+dfsg-2ubuntu0.2 [130 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qttranslations5-l10n all 5.15.3-1 [1,983 kB]\n",
      "Fetched 142 MB in 5s (27.6 MB/s)                 \u001b[0m\u001b[33m\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 31.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Extracting templates from packages: 100%\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libqt5core5a:amd64.\n",
      "(Reading database ... 127400 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libqt5core5a_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8Unpacking libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [..........................................................] \u001b8Selecting previously unselected package libevdev2:amd64.\n",
      "Preparing to unpack .../01-libevdev2_1.12.1+dfsg-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  3%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Selecting previously unselected package libmtdev1:amd64.\n",
      "Preparing to unpack .../02-libmtdev1_1.1.6-1build4_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Unpacking libmtdev1:amd64 (1.1.6-1build4) ...\n",
      "Selecting previously unselected package libgudev-1.0-0:amd64.\n",
      "Preparing to unpack .../03-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Selecting previously unselected package libwacom-common.\n",
      "Preparing to unpack .../04-libwacom-common_2.2.0-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libwacom-common (2.2.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Selecting previously unselected package libwacom9:amd64.\n",
      "Preparing to unpack .../05-libwacom9_2.2.0-1_amd64.deb ...\n",
      "Unpacking libwacom9:amd64 (2.2.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Selecting previously unselected package libinput-bin.\n",
      "Preparing to unpack .../06-libinput-bin_1.20.0-1ubuntu0.3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking libinput-bin (1.20.0-1ubuntu0.3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Selecting previously unselected package libinput10:amd64.\n",
      "Preparing to unpack .../07-libinput10_1.20.0-1ubuntu0.3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
      "Selecting previously unselected package libmd4c0:amd64.\n",
      "Preparing to unpack .../08-libmd4c0_0.4.8-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking libmd4c0:amd64 (0.4.8-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libqt5dbus5:amd64.\n",
      "Preparing to unpack .../09-libqt5dbus5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Unpacking libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Selecting previously unselected package libqt5network5:amd64.\n",
      "Preparing to unpack .../10-libqt5network5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
      "Unpacking libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libxcb-icccm4:amd64.\n",
      "Preparing to unpack .../11-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package libxcb-util1:amd64.\n",
      "Preparing to unpack .../12-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
      "Selecting previously unselected package libxcb-image0:amd64.\n",
      "Preparing to unpack .../13-libxcb-image0_0.4.0-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libxcb-keysyms1:amd64.\n",
      "Preparing to unpack .../14-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package libxcb-render-util0:amd64.\n",
      "Preparing to unpack .../15-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
      "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [##############............................................] \u001b8Selecting previously unselected package libxcb-xinerama0:amd64.\n",
      "Preparing to unpack .../16-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package libxcb-xinput0:amd64.\n",
      "Preparing to unpack .../17-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [################..........................................] \u001b8Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-xkb1:amd64.\n",
      "Preparing to unpack .../18-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
      "Preparing to unpack .../19-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libqt5gui5:amd64.\n",
      "Preparing to unpack .../20-libqt5gui5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
      "Unpacking libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Selecting previously unselected package libqt5widgets5:amd64.\n",
      "Preparing to unpack .../21-libqt5widgets5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package libqt5svg5:amd64.\n",
      "Preparing to unpack .../22-libqt5svg5_5.15.3-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8Unpacking libqt5svg5:amd64 (5.15.3-1) ...\n",
      "Selecting previously unselected package fluid-soundfont-gm.\n",
      "Preparing to unpack .../23-fluid-soundfont-gm_3.1-5.3_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Unpacking fluid-soundfont-gm (3.1-5.3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Selecting previously unselected package libinstpatch-1.0-2:amd64.\n",
      "Preparing to unpack .../24-libinstpatch-1.0-2_1.1.6-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Selecting previously unselected package libfluidsynth3:amd64.\n",
      "Preparing to unpack .../25-libfluidsynth3_2.2.5-1_amd64.deb ...\n",
      "Unpacking libfluidsynth3:amd64 (2.2.5-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package fluidsynth.\n",
      "Preparing to unpack .../26-fluidsynth_2.2.5-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Unpacking fluidsynth (2.2.5-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package libwacom-bin.\n",
      "Preparing to unpack .../27-libwacom-bin_2.2.0-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libwacom-bin (2.2.0-1) ...\n",
      "Selecting previously unselected package qsynth.\n",
      "Preparing to unpack .../28-qsynth_0.9.6-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking qsynth (0.9.6-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Selecting previously unselected package qt5-gtk-platformtheme:amd64.\n",
      "Preparing to unpack .../29-qt5-gtk-platformtheme_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Unpacking qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Selecting previously unselected package qttranslations5-l10n.\n",
      "Preparing to unpack .../30-qttranslations5-l10n_5.15.3-1_all.deb ...\n",
      "Unpacking qttranslations5-l10n (5.15.3-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [############################..............................] \u001b8Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [################################..........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [###################################.......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up qttranslations5-l10n (5.15.3-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libmtdev1:amd64 (1.1.6-1build4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libmd4c0:amd64 (0.4.8-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up fluid-soundfont-gm (3.1-5.3) ...\n",
      "update-alternatives: using /usr/share/sounds/sf2/FluidR3_GM.sf2 to provide /usr/share/sounds/sf2/default-GM.sf2 (default-GM.sf2) in auto mode\n",
      "update-alternatives: using /usr/share/sounds/sf2/FluidR3_GM.sf2 to provide /usr/share/sounds/sf3/default-GM.sf3 (default-GM.sf3) in auto mode\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 74%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 74%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libfluidsynth3:amd64 (2.2.5-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libwacom-common (2.2.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libwacom9:amd64 (2.2.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libinput-bin (1.20.0-1ubuntu0.3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [#################################################.........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up fluidsynth (2.2.5-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8Created symlink /etc/systemd/user/default.target.wants/fluidsynth.service → /usr/lib/systemd/user/fluidsynth.service.\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libwacom-bin (2.2.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [####################################################......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [#######################################################...] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up libqt5svg5:amd64 (5.15.3-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up qsynth (0.9.6-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [#########################################################.] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 99%]\u001b[49m\u001b[39m [#########################################################.] \u001b8Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!pip install midi2audio pydub\n",
    "!sudo apt update\n",
    "!sudo apt install -y fluidsynth libfluidsynth3 fluid-soundfont-gm ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T11:54:23.216556Z",
     "iopub.status.busy": "2025-06-23T11:54:23.216211Z",
     "iopub.status.idle": "2025-06-23T11:54:23.265615Z",
     "shell.execute_reply": "2025-06-23T11:54:23.264805Z",
     "shell.execute_reply.started": "2025-06-23T11:54:23.216531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from midi2audio import FluidSynth\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def midi_to_mp3(midi_path, mp3_path, soundfont=None):\n",
    "    wav_path = midi_path.replace(\".mid\", \".wav\")\n",
    "    fs = FluidSynth(soundfont) if soundfont else FluidSynth()\n",
    "    fs.midi_to_audio(midi_path, wav_path)\n",
    "\n",
    "    audio = AudioSegment.from_wav(wav_path)\n",
    "    audio.export(mp3_path, format=\"mp3\")\n",
    "\n",
    "    os.remove(wav_path)  # clean up\n",
    "    print(f\"Converted to MP3 → {mp3_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T11:59:08.674605Z",
     "iopub.status.busy": "2025-06-23T11:59:08.674169Z",
     "iopub.status.idle": "2025-06-23T11:59:09.004555Z",
     "shell.execute_reply": "2025-06-23T11:59:09.003756Z",
     "shell.execute_reply.started": "2025-06-23T11:59:08.674576Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to MP3 → /kaggle/working/generated_music.mp3\n"
     ]
    }
   ],
   "source": [
    "midi_to_mp3(\n",
    "  \"/kaggle/working/happy_generated_music.mid\",\n",
    "  \"/kaggle/working/generated_music.mp3\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 716027,
     "sourceId": 1246998,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7716580,
     "sourceId": 12246809,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
